{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "# get the dataset path from the environment variables\n",
    "dataset_path = os.environ.get(\"DATASET_PATH\")\n",
    "# get the articles path for the known publisher\n",
    "articles_path = os.path.join(dataset_path, \"articles\")\n",
    "# load the dataset into the notebook\n",
    "df = pd.read_csv(f\"{dataset_path}/article_info_V2.csv\", index_col=0, parse_dates=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(str):\n",
    "    # make the string lowercase\n",
    "    str = str.lower()\n",
    "    # remove all non-alphanumeric characters\n",
    "    str = re.sub(r\"[^\\w\\s]\", \"\", str)\n",
    "    # remove the leading and trailing spaces\n",
    "    return str.strip()\n",
    "\n",
    "def parse_list_from_string(str):\n",
    "    # split the string on each comma\n",
    "    raw_list = str.split(\",\")\n",
    "    # clean every string in the list\n",
    "    return list(map(clean_string, raw_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the author, type and keywords columns\n",
    "df.drop(columns=[\"Author\", \"Type\", \"Keywords\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with empty date values\n",
    "df.dropna(subset=[\"Date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the external dataset\n",
    "external_df = pd.read_csv(f\"{dataset_path}/other-articles.csv\", index_col=0, parse_dates=[1], sep=\";\")\n",
    "# remove the url column from the external dataset\n",
    "external_df.drop(columns=[\"Url\"], inplace=True)\n",
    "# add the external dataset to the main dataset\n",
    "df = pd.concat([df, external_df])\n",
    "# reset the index of the main dataset\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# clear the external dataset variable\n",
    "del external_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 3 duplicate entries.\n"
     ]
    }
   ],
   "source": [
    "# count the number of entries before duplicate removal\n",
    "entries_before_duplicate_removal = len(df)\n",
    "# remove duplicate entries by title and date\n",
    "df.drop_duplicates(subset=[\"Title\", \"Date\"], keep=\"first\", inplace=True)\n",
    "# count the number of entries after duplicate removal\n",
    "entries_after_duplicate_removal = len(df)\n",
    "# print removed duplicate count\n",
    "print(f\"Removed {entries_before_duplicate_removal - entries_after_duplicate_removal} duplicate entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the string lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of values in the Tags column is <class 'str'>.\n",
      "The type of values in the Tags column is <class 'list'>.\n"
     ]
    }
   ],
   "source": [
    "# show the tags column before parsing\n",
    "print(f\"The type of values in the Tags column is {type(df.loc[0, 'Tags'])}.\")\n",
    "# convert the raw string values of the Tags column to lists of strings\n",
    "df[\"Tags\"] = df[\"Tags\"].apply(parse_list_from_string)\n",
    "# show the tags column after parsing\n",
    "print(f\"The type of values in the Tags column is {type(df.loc[0, 'Tags'])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define article loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article by title\n",
    "def load_article(title, load_contents=True):\n",
    "    # create file name from title\n",
    "    file_name = f\"{title}.txt\"\n",
    "    # get the path of the article\n",
    "    file_path = os.path.join(articles_path, file_name)\n",
    "    # read the article\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    # return the contents of the article if requested\n",
    "    if load_contents:\n",
    "        # read the contents of the article\n",
    "        contents = file.read()\n",
    "        # close the file\n",
    "        file.close()\n",
    "        # return the contents of the article\n",
    "        return contents\n",
    "    # close the file\n",
    "    file.close()\n",
    "    # return the article path\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove articles which cannot be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 files could not be loaded by title!\n"
     ]
    }
   ],
   "source": [
    "# initialize error count to 0 \n",
    "err_count = 0\n",
    "\n",
    "# iterate over dataset with index\n",
    "for index, row in df.iterrows():\n",
    "    # get the title of the article\n",
    "    title = row[\"Title\"]\n",
    "    try:\n",
    "        # attempt to load the article\n",
    "        article = load_article(title)\n",
    "    except:\n",
    "        # if the article cannot be loaded, increment the error count\n",
    "        err_count += 1\n",
    "        # remove row from dataset\n",
    "        df.drop(index, inplace=True)\n",
    "        # continue to next row\n",
    "        continue\n",
    "\n",
    "# reset the index of the main dataset\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print the number of errors\n",
    "print(f\"{err_count} files could not be loaded by title!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define article cleaning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the small english spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace characters like é, ë, ï, etc. with their corresponding characters\n",
    "def remove_accented_characters(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all newline characters\n",
    "def remove_newlines(text):\n",
    "    text = text.replace(\"\\n\\n\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\\r\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace shortened grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace shortened grammar with full grammar\n",
    "def replace_grammar(text):\n",
    "    text = text.replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\"he's\", \"he is\")\n",
    "    text = text.replace(\"she's\", \"she is\")\n",
    "    text = text.replace(\"'s\", \" its\")\n",
    "    text = text.replace(\"'t\", \" not\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    text = text.replace(\"'ve\", \" have\")\n",
    "    text = text.replace(\"'d\", \" would\")\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove double whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove double whitespace characters\n",
    "def remove_double_whitespace(text):\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove noise from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise from text\n",
    "def remove_noise(text):\n",
    "    # remove newline characters\n",
    "    text = remove_newlines(text)\n",
    "    # replace short grammar with full grammar\n",
    "    text = replace_grammar(text)\n",
    "    # remove accented characters\n",
    "    text = remove_accented_characters(text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # remove digits\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # remove double whitespace characters\n",
    "    text = remove_double_whitespace(text)\n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    # return the cleaned text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "def tokenize(text):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words from the text\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the text\n",
    "def lemmatize(tokens):\n",
    "    return [token.lemma_ for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main preprocessing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main preprocessing method which calls all cleaning methods\n",
    "def preprocess_text(text):\n",
    "    text = remove_noise(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stop_words(tokens)\n",
    "    tokens = lemmatize(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Moïse', 11),\n",
       " ('drug', 11),\n",
       " ('Times', 10),\n",
       " ('Haiti', 9),\n",
       " ('cocaine', 8),\n",
       " ('according', 7),\n",
       " ('DEA', 7),\n",
       " ('trafficking', 6),\n",
       " ('officials', 5),\n",
       " ('airstrips', 5)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "\n",
    "keyword = []\n",
    "stopwords = list(STOP_WORDS)\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN','VERB']\n",
    "for token in doc:\n",
    "    if(token.text in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "    if(token.pos_ in pos_tag):\n",
    "        keyword.append(token.text)\n",
    "\n",
    "freq_word = Counter(keyword)\n",
    "freq_word.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Moïse', 1.0),\n",
       " ('drug', 1.0),\n",
       " ('Times', 0.9090909090909091),\n",
       " ('Haiti', 0.8181818181818182),\n",
       " ('cocaine', 0.7272727272727273)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "for word in freq_word.keys():\n",
    "    freq_word[word] = (freq_word[word]/max_freq)\n",
    "freq_word.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{An explosive new report suggests that the high-profile assassination of Jovenel Moïse may have been related to a crackdown on drug trafficking and a list he was compiling of Haitian business and political elites involved in the trade, adding yet another theory to the possible motives for the former president's killing.: 6.636363636363634, \n",
      "\n",
      "Before he was shot dead, President Moïse had planned to hand the names over to the US government, according to a New York Times report published December 12.: 4.2727272727272725, The Times spoke to four senior Haitian advisers and officials who had knowledge of the document.: 2.3636363636363633, Unnamed officials also told the Times that the hitmen had confessed to ransacking Moïse's house in search of the list.: 3.3636363636363633, \n",
      "\n",
      "\"The president had ordered the officials to spare no one, not even the power brokers who had helped propel him into office,\" the Times reported.: 2.7272727272727266, \n",
      "\n",
      "A “central figure” included on the list, according to the Times, was businessman Charles Saint-Rémy, alias “Kiko.”: 3.2727272727272725, The Times previously reported that US anti-drug officials who had worked in Haiti had suspected Saint-Rémy’s involvement in drug trafficking.: 6.545454545454545, \n",
      "\n",
      "In 2015, Saint-Rémy allegedly met with senior Drug Enforcement Administration (DEA) officials, raising questions of corruption, according to Keith McNichols, a former DEA agent who was investigating the smuggling of hundreds of kilograms of cocaine and heroin from Colombia to Haiti.: 6.8181818181818175, McNichols, and another DEA whistleblower, spoke out about how that investigation was grossly mishandled.: 1.2727272727272725, \n",
      "\n",
      "Saint-Rémy – who responded \"no, no, no\" to the Times when asked about alleged links to drug trafficking – is the brother-in-law of former President Michel Martelly, a close friend of current Prime Minister Ariel Henry.: 4.999999999999999, Haiti's former chief prosecutor previously accused Henry of being connected to Moïse’s murder.: 2.454545454545454, \n",
      "\n",
      "The dossier wasn’t the only move Moïse's allegedly made against drug trafficking.: 2.6363636363636362, In mid-2021, the DEA reportedly made Moïse aware of two clandestine airstrips used to receive drug flights in an area north of the capital, Port-au-Prince.: 4.181818181818182, Moïse ordered one of the airstrips destroyed, but local authorities reportedly refused to do so, according to the Times report.: 4.181818181818182, \n",
      "\n",
      "Earlier that year, a close ally of the former president also allegedly ordered a crackdown on the country’s eel industry, which is used “as a way to launder illicit profits,” the Times report said.: 3.3636363636363633, \n",
      "\n",
      "These actions and the list he supposedly compiled were just one part of a “broader series of clashes Moïse had with powerful political and business figures, some suspected of narcotics and arms trafficking,” the Times reported.: 4.727272727272726, \n",
      "\n",
      "InSight Crime Analysis\n",
      "\n",
      "The latest reporting adds one more theory to the possible motives for President Moïse’s assassination.: 2.8181818181818183, Yet his anti-drug crusade was never all that ambitious in a country that does not play a major role in the regional cocaine trade.: 3.454545454545454, \n",
      "\n",
      "Aside from a few isolated arrests and extraditions, Haiti's security forces failed to capture any major drug traffickers - or the powerful elites protecting them - under Moïse’s watch.: 4.909090909090908, Arguably the last major blow to drug trafficking in Haiti came with the arrest of Guy Philippe, who the DEA tried to capture in 2007 before he was later convicted in 2017 of laundering drug money in the United States.: 5.727272727272727, \n",
      "\n",
      "What's more, just eight percent of cocaine departing South America transited through the eastern Caribbean corridor via either Haiti or the Dominican Republic in 2019, according to the DEA’s 2020 National Drug Threat Assessment.: 4.909090909090908, Haiti's neighbor, the Dominican Republic, has emerged as a much bigger player in the transnational cocaine trade, thanks to its role as a regional hub for container ships, especially those arriving from Venezuela.: 3.545454545454544, \n",
      "\n",
      "(Graphic courtesy of the DEA's 2020 Drug Threat Assessment)\n",
      "\n",
      "To be sure, in 2019, Haitian police seized a total of just 10 kilograms of cocaine.: 3.2727272727272725, In 2020, the number rose to 103 kilograms, according to the State Department’s 2021 International Narcotics Control Strategy Report (INCSR).: 1.8181818181818175, That said, the low seizure figures are likely more of an indication of the Haitian police’s woeful anti-narcotics operations, suggesting that the amount of cocaine passing through could be higher.: 2.818181818181818, \n",
      "\n",
      "Indeed, notorious drug traffickers like Bedouin “Jaques” Ketant have in the past relied on Haiti as a transit point for large cocaine shipments.: 3.636363636363636, But the vast majority of cocaine heading north departs South America via the Eastern Pacific route off the shores of Colombia and Ecuador.: 2.272727272727272, The drugs then pass through Central America before continuing on to Mexico and eventually the United States.: 1.090909090909091, \n",
      "\n",
      "In addition, clandestine airstrips like those reportedly targeted by Moïse do not abound in Haiti, as they do in other nations that serve as major aerial passageways, such as Guatemala and Honduras.: 3.727272727272727, In Honduras, the armed forces have destroyed 21 airstrips this year alone, according to local media reports.: 2.3636363636363633, Since 2014, security forces there have uncovered more than 320 such airstrips.: 1.0, \n",
      "\n",
      "InSight Crime investigator Sergio Saffon contributed reporting to this article.*: 1.0}\n"
     ]
    }
   ],
   "source": [
    "sent_strength = {}\n",
    "for sent in doc.sents:\n",
    "    for word in sent:\n",
    "        if word.text in freq_word.keys():\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent]+=freq_word[word.text]\n",
    "            else:\n",
    "                sent_strength[sent]=freq_word[word.text]\n",
    "print(sent_strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "In 2015, Saint-Rémy allegedly met with senior Drug Enforcement Administration (DEA) officials, raising questions of corruption, according to Keith McNichols, a former DEA agent who was investigating the smuggling of hundreds of kilograms of cocaine and heroin from Colombia to Haiti. An explosive new report suggests that the high-profile assassination of Jovenel Moïse may have been related to a crackdown on drug trafficking and a list he was compiling of Haitian business and political elites involved in the trade, adding yet another theory to the possible motives for the former president's killing. The Times previously reported that US anti-drug officials who had worked in Haiti had suspected Saint-Rémy’s involvement in drug trafficking.\n"
     ]
    }
   ],
   "source": [
    "summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)\n",
    "final_sentences = [ w.text for w in summarized_sentences ]\n",
    "summary = ' '.join(final_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\ict\\DSAI\\main.ipynb Cell 47'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000061?line=4'>5</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgoogle/pegasus-xsum\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000061?line=5'>6</a>\u001b[0m torch_device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000061?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m PegasusTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000061?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m PegasusForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(model_name)\u001b[39m.\u001b[39mto(torch_device)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "# Configure model\n",
    "model_name = 'google/pegasus-xsum'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### commented for now - Preprocess all articles and append them to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 1 of 10180 articles.\n",
      "Preprocessed 2 of 10180 articles.\n",
      "Preprocessed 3 of 10180 articles.\n",
      "Preprocessed 4 of 10180 articles.\n",
      "Preprocessed 5 of 10180 articles.\n",
      "Preprocessed 6 of 10180 articles.\n",
      "Preprocessed 7 of 10180 articles.\n",
      "Preprocessed 8 of 10180 articles.\n",
      "Preprocessed 9 of 10180 articles.\n",
      "Preprocessed 10 of 10180 articles.\n",
      "Preprocessed 11 of 10180 articles.\n",
      "Preprocessed 12 of 10180 articles.\n",
      "Preprocessed 13 of 10180 articles.\n",
      "Preprocessed 14 of 10180 articles.\n",
      "Preprocessed 15 of 10180 articles.\n",
      "Preprocessed 16 of 10180 articles.\n",
      "Preprocessed 17 of 10180 articles.\n",
      "Preprocessed 18 of 10180 articles.\n",
      "Preprocessed 19 of 10180 articles.\n",
      "Preprocessed 20 of 10180 articles.\n",
      "Preprocessed 21 of 10180 articles.\n",
      "Preprocessed 22 of 10180 articles.\n",
      "Preprocessed 23 of 10180 articles.\n",
      "Preprocessed 24 of 10180 articles.\n",
      "Preprocessed 25 of 10180 articles.\n",
      "Preprocessed 26 of 10180 articles.\n",
      "Preprocessed 27 of 10180 articles.\n",
      "Preprocessed 28 of 10180 articles.\n",
      "Preprocessed 29 of 10180 articles.\n",
      "Preprocessed 30 of 10180 articles.\n",
      "Preprocessed 31 of 10180 articles.\n",
      "Preprocessed 32 of 10180 articles.\n",
      "Preprocessed 33 of 10180 articles.\n",
      "Preprocessed 34 of 10180 articles.\n",
      "Preprocessed 35 of 10180 articles.\n",
      "Preprocessed 36 of 10180 articles.\n",
      "Preprocessed 37 of 10180 articles.\n",
      "Preprocessed 38 of 10180 articles.\n",
      "Preprocessed 39 of 10180 articles.\n",
      "Preprocessed 40 of 10180 articles.\n",
      "Preprocessed 41 of 10180 articles.\n",
      "Preprocessed 42 of 10180 articles.\n",
      "Preprocessed 43 of 10180 articles.\n",
      "Preprocessed 44 of 10180 articles.\n",
      "Preprocessed 45 of 10180 articles.\n",
      "Preprocessed 46 of 10180 articles.\n",
      "Preprocessed 47 of 10180 articles.\n",
      "Preprocessed 48 of 10180 articles.\n",
      "Preprocessed 49 of 10180 articles.\n",
      "Preprocessed 50 of 10180 articles.\n",
      "Preprocessed 51 of 10180 articles.\n",
      "Preprocessed 52 of 10180 articles.\n",
      "Preprocessed 53 of 10180 articles.\n",
      "Preprocessed 54 of 10180 articles.\n",
      "Preprocessed 55 of 10180 articles.\n",
      "Preprocessed 56 of 10180 articles.\n",
      "Preprocessed 57 of 10180 articles.\n",
      "Preprocessed 58 of 10180 articles.\n",
      "Preprocessed 59 of 10180 articles.\n",
      "Preprocessed 60 of 10180 articles.\n",
      "Preprocessed 61 of 10180 articles.\n",
      "Preprocessed 62 of 10180 articles.\n",
      "Preprocessed 63 of 10180 articles.\n",
      "Preprocessed 64 of 10180 articles.\n",
      "Preprocessed 65 of 10180 articles.\n",
      "Preprocessed 66 of 10180 articles.\n",
      "Preprocessed 67 of 10180 articles.\n",
      "Preprocessed 68 of 10180 articles.\n",
      "Preprocessed 69 of 10180 articles.\n",
      "Preprocessed 70 of 10180 articles.\n",
      "Preprocessed 71 of 10180 articles.\n",
      "Preprocessed 72 of 10180 articles.\n",
      "Preprocessed 73 of 10180 articles.\n",
      "Preprocessed 74 of 10180 articles.\n",
      "Preprocessed 75 of 10180 articles.\n",
      "Preprocessed 76 of 10180 articles.\n",
      "Preprocessed 77 of 10180 articles.\n",
      "Preprocessed 78 of 10180 articles.\n",
      "Preprocessed 79 of 10180 articles.\n",
      "Preprocessed 80 of 10180 articles.\n",
      "Preprocessed 81 of 10180 articles.\n",
      "Preprocessed 82 of 10180 articles.\n",
      "Preprocessed 83 of 10180 articles.\n",
      "Preprocessed 84 of 10180 articles.\n",
      "Preprocessed 85 of 10180 articles.\n",
      "Preprocessed 86 of 10180 articles.\n",
      "Preprocessed 87 of 10180 articles.\n",
      "Preprocessed 88 of 10180 articles.\n",
      "Preprocessed 89 of 10180 articles.\n",
      "Preprocessed 90 of 10180 articles.\n",
      "Preprocessed 91 of 10180 articles.\n",
      "Preprocessed 92 of 10180 articles.\n",
      "Preprocessed 93 of 10180 articles.\n",
      "Preprocessed 94 of 10180 articles.\n",
      "Preprocessed 95 of 10180 articles.\n",
      "Preprocessed 96 of 10180 articles.\n",
      "Preprocessed 97 of 10180 articles.\n",
      "Preprocessed 98 of 10180 articles.\n",
      "Preprocessed 99 of 10180 articles.\n",
      "Preprocessed 100 of 10180 articles.\n",
      "Preprocessed 101 of 10180 articles.\n",
      "Preprocessed 102 of 10180 articles.\n",
      "Preprocessed 103 of 10180 articles.\n",
      "Preprocessed 104 of 10180 articles.\n",
      "Preprocessed 105 of 10180 articles.\n",
      "Preprocessed 106 of 10180 articles.\n",
      "Preprocessed 107 of 10180 articles.\n",
      "Preprocessed 108 of 10180 articles.\n",
      "Preprocessed 109 of 10180 articles.\n",
      "Preprocessed 110 of 10180 articles.\n",
      "Preprocessed 111 of 10180 articles.\n",
      "Preprocessed 112 of 10180 articles.\n",
      "Preprocessed 113 of 10180 articles.\n",
      "Preprocessed 114 of 10180 articles.\n",
      "Preprocessed 115 of 10180 articles.\n",
      "Preprocessed 116 of 10180 articles.\n",
      "Preprocessed 117 of 10180 articles.\n",
      "Preprocessed 118 of 10180 articles.\n",
      "Preprocessed 119 of 10180 articles.\n",
      "Preprocessed 120 of 10180 articles.\n",
      "Preprocessed 121 of 10180 articles.\n",
      "Preprocessed 122 of 10180 articles.\n",
      "Preprocessed 123 of 10180 articles.\n",
      "Preprocessed 124 of 10180 articles.\n",
      "Preprocessed 125 of 10180 articles.\n",
      "Preprocessed 126 of 10180 articles.\n",
      "Preprocessed 127 of 10180 articles.\n",
      "Preprocessed 128 of 10180 articles.\n",
      "Preprocessed 129 of 10180 articles.\n",
      "Preprocessed 130 of 10180 articles.\n",
      "Preprocessed 131 of 10180 articles.\n",
      "Preprocessed 132 of 10180 articles.\n",
      "Preprocessed 133 of 10180 articles.\n",
      "Preprocessed 134 of 10180 articles.\n",
      "Preprocessed 135 of 10180 articles.\n",
      "Preprocessed 136 of 10180 articles.\n",
      "Preprocessed 137 of 10180 articles.\n",
      "Preprocessed 138 of 10180 articles.\n",
      "Preprocessed 139 of 10180 articles.\n",
      "Preprocessed 140 of 10180 articles.\n",
      "Preprocessed 141 of 10180 articles.\n",
      "Preprocessed 142 of 10180 articles.\n",
      "Preprocessed 143 of 10180 articles.\n",
      "Preprocessed 144 of 10180 articles.\n",
      "Preprocessed 145 of 10180 articles.\n",
      "Preprocessed 146 of 10180 articles.\n",
      "Preprocessed 147 of 10180 articles.\n",
      "Preprocessed 148 of 10180 articles.\n",
      "Preprocessed 149 of 10180 articles.\n",
      "Preprocessed 150 of 10180 articles.\n",
      "Preprocessed 151 of 10180 articles.\n",
      "Preprocessed 152 of 10180 articles.\n",
      "Preprocessed 153 of 10180 articles.\n",
      "Preprocessed 154 of 10180 articles.\n",
      "Preprocessed 155 of 10180 articles.\n",
      "Preprocessed 156 of 10180 articles.\n",
      "Preprocessed 157 of 10180 articles.\n",
      "Preprocessed 158 of 10180 articles.\n",
      "Preprocessed 159 of 10180 articles.\n",
      "Preprocessed 160 of 10180 articles.\n",
      "Preprocessed 161 of 10180 articles.\n",
      "Preprocessed 162 of 10180 articles.\n",
      "Preprocessed 163 of 10180 articles.\n",
      "Preprocessed 164 of 10180 articles.\n",
      "Preprocessed 165 of 10180 articles.\n",
      "Preprocessed 166 of 10180 articles.\n",
      "Preprocessed 167 of 10180 articles.\n",
      "Preprocessed 168 of 10180 articles.\n",
      "Preprocessed 169 of 10180 articles.\n",
      "Preprocessed 170 of 10180 articles.\n",
      "Preprocessed 171 of 10180 articles.\n",
      "Preprocessed 172 of 10180 articles.\n",
      "Preprocessed 173 of 10180 articles.\n",
      "Preprocessed 174 of 10180 articles.\n",
      "Preprocessed 175 of 10180 articles.\n",
      "Preprocessed 176 of 10180 articles.\n",
      "Preprocessed 177 of 10180 articles.\n",
      "Preprocessed 178 of 10180 articles.\n",
      "Preprocessed 179 of 10180 articles.\n",
      "Preprocessed 180 of 10180 articles.\n",
      "Preprocessed 181 of 10180 articles.\n",
      "Preprocessed 182 of 10180 articles.\n",
      "Preprocessed 183 of 10180 articles.\n",
      "Preprocessed 184 of 10180 articles.\n",
      "Preprocessed 185 of 10180 articles.\n",
      "Preprocessed 186 of 10180 articles.\n",
      "Preprocessed 187 of 10180 articles.\n",
      "Preprocessed 188 of 10180 articles.\n",
      "Preprocessed 189 of 10180 articles.\n",
      "Preprocessed 190 of 10180 articles.\n",
      "Preprocessed 191 of 10180 articles.\n",
      "Preprocessed 192 of 10180 articles.\n",
      "Preprocessed 193 of 10180 articles.\n",
      "Preprocessed 194 of 10180 articles.\n",
      "Preprocessed 195 of 10180 articles.\n",
      "Preprocessed 196 of 10180 articles.\n",
      "Preprocessed 197 of 10180 articles.\n",
      "Preprocessed 198 of 10180 articles.\n",
      "Preprocessed 199 of 10180 articles.\n",
      "Preprocessed 200 of 10180 articles.\n",
      "Preprocessed 201 of 10180 articles.\n",
      "Preprocessed 202 of 10180 articles.\n",
      "Preprocessed 203 of 10180 articles.\n",
      "Preprocessed 204 of 10180 articles.\n",
      "Preprocessed 205 of 10180 articles.\n",
      "Preprocessed 206 of 10180 articles.\n",
      "Preprocessed 207 of 10180 articles.\n",
      "Preprocessed 208 of 10180 articles.\n",
      "Preprocessed 209 of 10180 articles.\n",
      "Preprocessed 210 of 10180 articles.\n",
      "Preprocessed 211 of 10180 articles.\n",
      "Preprocessed 212 of 10180 articles.\n",
      "Preprocessed 213 of 10180 articles.\n",
      "Preprocessed 214 of 10180 articles.\n",
      "Preprocessed 215 of 10180 articles.\n",
      "Preprocessed 216 of 10180 articles.\n",
      "Preprocessed 217 of 10180 articles.\n",
      "Preprocessed 218 of 10180 articles.\n",
      "Preprocessed 219 of 10180 articles.\n",
      "Preprocessed 220 of 10180 articles.\n",
      "Preprocessed 221 of 10180 articles.\n",
      "Preprocessed 222 of 10180 articles.\n",
      "Preprocessed 223 of 10180 articles.\n",
      "Preprocessed 224 of 10180 articles.\n",
      "Preprocessed 225 of 10180 articles.\n",
      "Preprocessed 226 of 10180 articles.\n",
      "Preprocessed 227 of 10180 articles.\n",
      "Preprocessed 228 of 10180 articles.\n",
      "Preprocessed 229 of 10180 articles.\n",
      "Preprocessed 230 of 10180 articles.\n",
      "Preprocessed 231 of 10180 articles.\n",
      "Preprocessed 232 of 10180 articles.\n",
      "Preprocessed 233 of 10180 articles.\n",
      "Preprocessed 234 of 10180 articles.\n",
      "Preprocessed 235 of 10180 articles.\n",
      "Preprocessed 236 of 10180 articles.\n",
      "Preprocessed 237 of 10180 articles.\n",
      "Preprocessed 238 of 10180 articles.\n",
      "Preprocessed 239 of 10180 articles.\n",
      "Preprocessed 240 of 10180 articles.\n",
      "Preprocessed 241 of 10180 articles.\n",
      "Preprocessed 242 of 10180 articles.\n",
      "Preprocessed 243 of 10180 articles.\n",
      "Preprocessed 244 of 10180 articles.\n",
      "Preprocessed 245 of 10180 articles.\n",
      "Preprocessed 246 of 10180 articles.\n",
      "Preprocessed 247 of 10180 articles.\n",
      "Preprocessed 248 of 10180 articles.\n",
      "Preprocessed 249 of 10180 articles.\n",
      "Preprocessed 250 of 10180 articles.\n",
      "Preprocessed 251 of 10180 articles.\n",
      "Preprocessed 252 of 10180 articles.\n",
      "Preprocessed 253 of 10180 articles.\n",
      "Preprocessed 254 of 10180 articles.\n",
      "Preprocessed 255 of 10180 articles.\n",
      "Preprocessed 256 of 10180 articles.\n",
      "Preprocessed 257 of 10180 articles.\n",
      "Preprocessed 258 of 10180 articles.\n",
      "Preprocessed 259 of 10180 articles.\n",
      "Preprocessed 260 of 10180 articles.\n",
      "Preprocessed 261 of 10180 articles.\n",
      "Preprocessed 262 of 10180 articles.\n",
      "Preprocessed 263 of 10180 articles.\n",
      "Preprocessed 264 of 10180 articles.\n",
      "Preprocessed 265 of 10180 articles.\n",
      "Preprocessed 266 of 10180 articles.\n",
      "Preprocessed 267 of 10180 articles.\n",
      "Preprocessed 268 of 10180 articles.\n",
      "Preprocessed 269 of 10180 articles.\n",
      "Preprocessed 270 of 10180 articles.\n",
      "Preprocessed 271 of 10180 articles.\n",
      "Preprocessed 272 of 10180 articles.\n",
      "Preprocessed 273 of 10180 articles.\n",
      "Preprocessed 274 of 10180 articles.\n",
      "Preprocessed 275 of 10180 articles.\n",
      "Preprocessed 276 of 10180 articles.\n",
      "Preprocessed 277 of 10180 articles.\n",
      "Preprocessed 278 of 10180 articles.\n",
      "Preprocessed 279 of 10180 articles.\n",
      "Preprocessed 280 of 10180 articles.\n",
      "Preprocessed 281 of 10180 articles.\n",
      "Preprocessed 282 of 10180 articles.\n",
      "Preprocessed 283 of 10180 articles.\n",
      "Preprocessed 284 of 10180 articles.\n",
      "Preprocessed 285 of 10180 articles.\n",
      "Preprocessed 286 of 10180 articles.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\ict\\DSAI\\main.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000043?line=5'>6</a>\u001b[0m article_contents \u001b[39m=\u001b[39m load_article(title)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000043?line=6'>7</a>\u001b[0m \u001b[39m# preprocess the text\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000043?line=7'>8</a>\u001b[0m article_contents \u001b[39m=\u001b[39m preprocess_text(article_contents)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000043?line=8'>9</a>\u001b[0m \u001b[39m# join all tokens together\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000043?line=9'>10</a>\u001b[0m article_contents \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(article_contents)\n",
      "\u001b[1;32mc:\\ict\\DSAI\\main.ipynb Cell 42'\u001b[0m in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000041?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(text):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000041?line=2'>3</a>\u001b[0m     text \u001b[39m=\u001b[39m remove_noise(text)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000041?line=3'>4</a>\u001b[0m     tokens \u001b[39m=\u001b[39m tokenize(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000041?line=4'>5</a>\u001b[0m     tokens \u001b[39m=\u001b[39m remove_stop_words(tokens)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000041?line=5'>6</a>\u001b[0m     tokens \u001b[39m=\u001b[39m lemmatize(tokens)\n",
      "\u001b[1;32mc:\\ict\\DSAI\\main.ipynb Cell 36'\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000035?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(text):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ict/DSAI/main.ipynb#ch0000035?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m nlp(text)\n",
      "File \u001b[1;32mc:\\Users\\Vermeer\\.pyenv\\pyenv-win\\versions\\3.10.4\\lib\\site-packages\\spacy\\language.py:1017\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vermeer/.pyenv/pyenv-win/versions/3.10.4/lib/site-packages/spacy/language.py?line=1014'>1015</a>\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   <a href='file:///c%3A/Users/Vermeer/.pyenv/pyenv-win/versions/3.10.4/lib/site-packages/spacy/language.py?line=1015'>1016</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Vermeer/.pyenv/pyenv-win/versions/3.10.4/lib/site-packages/spacy/language.py?line=1016'>1017</a>\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vermeer/.pyenv/pyenv-win/versions/3.10.4/lib/site-packages/spacy/language.py?line=1017'>1018</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   <a href='file:///c%3A/Users/Vermeer/.pyenv/pyenv-win/versions/3.10.4/lib/site-packages/spacy/language.py?line=1018'>1019</a>\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Vermeer/.pyenv/pyenv-win/versions/3.10.4/lib/site-packages/spacy/language.py?line=1019'>1020</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# iterate over every row with index\n",
    "for index, row in df.iterrows():\n",
    "    # get the current title\n",
    "    title = row[\"Title\"]\n",
    "    # get the current text\n",
    "    article_contents = load_article(title)\n",
    "    # preprocess the text\n",
    "    article_contents = preprocess_text(article_contents)\n",
    "    # join all tokens together\n",
    "    article_contents = \" \".join(article_contents)\n",
    "    # make the text lowercase\n",
    "    article_contents = article_contents.lower()\n",
    "    # add the preprocessed text to the dataset\n",
    "    df.loc[index, \"Preprocessed_Text\"] = article_contents\n",
    "    # show the progress\n",
    "    print(f\"Preprocessed {index + 1} of {len(df)} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporary visualization of the preprocessed data of one article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el salvador attorney general raul melara call evidence witness statement unseal corruption case information available public seemingly transparency melara demand come witness implicate deputy attorney general allan hernandez bribery scheme s land prosecutor jail spokesman attorney general office say measure melara anticorruption fight unseal record expose witness testify deputy alleged corruption salvadoran law allow sealing evidence protect victim witness alike addition withhold identity minor age sealing request prosecutor case judge officially impose regulation certain portion case file withhold view specific statement witness receive benefit exchange testimony people right know request seal lift corruptionrelate case attorney general melara write tweet el salvador news profile majority open corruption case recent year evidence seal identity witness withhold request attorney general office measure prove vital recent embezzling case president antonio saca mauricio fune melaras predecessor douglas melendez rely heavily testimony witness receive legal benefit include immunity exchange information detail crime allegedly commit head state current prosecution attorney general luis martinez allegedly turn prosecutor office paytoplay shop require witness information case deputy attorney general hernandez accuse official receive cash prominent businessman enrique rais rais accuse bribe member attorney general office order favor legal proceeding initiate insight crime analysis unclear melaras motive unseal information country corruption case primarily not thoroughly explain eliminate sealing evidence corruption case reveal detail scope corruption el salvador expose public official provide information involvement corruption work protection president unsealing ruin ongoing investigation melara know indepth coverage elite organize crime example case attorney general martinez base large extent witness minor role corruption scheme agree disclose know exchange reduced sentence avoid charge call unsealing corruption evidence melara say attempt prosecute people receive benefit exchange testimony unclear open investigation official name come testimony include deputy attorney general allan hernandez defend public despite fact witness official document allegedly link hernandez past corruption network prosecutor office\n"
     ]
    }
   ],
   "source": [
    "# define the article index\n",
    "article_index = 1234\n",
    "# get the title of an article\n",
    "title = df[\"Title\"][article_index]\n",
    "# load the body of the article\n",
    "article_contents = load_article(title)\n",
    "# preprocess the text\n",
    "article_contents = preprocess_text(article_contents)\n",
    "# show the progress\n",
    "total_text = \" \".join(article_contents)\n",
    "print(total_text)\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# wc=WordCloud(max_font_size=60).generate(total_text)\n",
    "# plt.figure(figsize=(16, 12))\n",
    "# plt.imshow(wc, interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define one hot encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from a 2D array of strings\n",
    "def get_unique_value_frequency(df_column):\n",
    "    # create a dictionary to store the unique values\n",
    "    unique_values = {}\n",
    "    # iterate over the column\n",
    "    for value_list in df_column:\n",
    "        # iterate over the values in the list\n",
    "        for value in value_list:\n",
    "            if value not in unique_values:\n",
    "                # if the value is not in the dictionary, add it\n",
    "                unique_values[value] = 1\n",
    "            else:\n",
    "                # if the value is in the dictionary, increment the value\n",
    "                unique_values[value] += 1\n",
    "    # return the dictionary of unique values\n",
    "    return unique_values\n",
    "\n",
    "# check if a list contains a certain word and returns a binary boolean value\n",
    "def list_has_word(l, word):\n",
    "    return word in l and 1 or 0\n",
    "\n",
    "# one hot encode a dataframe's column that contains lists of strings in each value\n",
    "def custom_one_hot_encoding(df, column_name, prefix=None, prefix_sep=\"_\"):\n",
    "    # create a dictionary to store the one hot encoded columns\n",
    "    one_hot_encoded_columns = {}\n",
    "    # get the unique values from the column\n",
    "    unique_values = get_unique_value_frequency(df[column_name])\n",
    "    # iterate over the unique values\n",
    "    for unique_value in unique_values:\n",
    "        # create a clean string of the unique value\n",
    "        clean_unique_value = unique_value.replace(\" \", \"_\")\n",
    "        # create a new column name\n",
    "        new_column_name = prefix and f\"{prefix}{prefix_sep}{clean_unique_value}\" or f\"{column_name}{prefix_sep}{clean_unique_value}\"\n",
    "        # one hot encode the column using the current unique value\n",
    "        ohe_list = df[column_name].apply(lambda l: list_has_word(l, unique_value))\n",
    "        # add the new list to the dictionary\n",
    "        one_hot_encoded_columns[new_column_name] = ohe_list\n",
    "    # return a new dataframe with the one hot encoded columns\n",
    "    return pd.DataFrame(one_hot_encoded_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute one hot encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the tags column of the dataframe\n",
    "ohe_tags_df = custom_one_hot_encoding(df, \"Tags\", \"tag\")\n",
    "# merge the one hot encoded tags dataframe with the main dataframe by index\n",
    "df = df.join(ohe_tags_df)\n",
    "# drop the tags column from the main dataframe\n",
    "df.drop(columns=[\"Tags\"], inplace=True)\n",
    "# delete the one hot encoded dataframe variable\n",
    "del ohe_tags_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc03c57ad7b86a0ef884a1652e2e233be898816187802410f759e457b32a702a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
