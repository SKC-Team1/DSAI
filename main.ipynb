{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "# get the dataset path from the environment variables\n",
    "dataset_path = os.environ.get(\"DATASET_PATH\")\n",
    "# get the articles path for the known publisher\n",
    "articles_path = os.path.join(dataset_path, \"articles\")\n",
    "# load the dataset into the notebook\n",
    "df = pd.read_csv(f\"{dataset_path}/article_info_V2.csv\", index_col=0, parse_dates=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(str):\n",
    "    # make the string lowercase\n",
    "    str = str.lower()\n",
    "    # remove all non-alphanumeric characters\n",
    "    str = re.sub(r\"[^\\w\\s]\", \"\", str)\n",
    "    # remove the leading and trailing spaces\n",
    "    return str.strip()\n",
    "\n",
    "def parse_list_from_string(str):\n",
    "    # split the string on each comma\n",
    "    raw_list = str.split(\",\")\n",
    "    # clean every string in the list\n",
    "    return list(map(clean_string, raw_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the author, type and keywords columns\n",
    "df.drop(columns=[\"Author\", \"Type\", \"Keywords\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with empty date values\n",
    "df.dropna(subset=[\"Date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the external dataset\n",
    "external_df = pd.read_csv(f\"{dataset_path}/other-articles.csv\", index_col=0, parse_dates=[1], sep=\";\")\n",
    "# remove the url column from the external dataset\n",
    "external_df.drop(columns=[\"Url\"], inplace=True)\n",
    "# add the external dataset to the main dataset\n",
    "df = pd.concat([df, external_df])\n",
    "# reset the index of the main dataset\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# clear the external dataset variable\n",
    "del external_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of entries before duplicate removal\n",
    "entries_before_duplicate_removal = len(df)\n",
    "# remove duplicate entries by title and date\n",
    "df.drop_duplicates(subset=[\"Title\", \"Date\"], keep=\"first\", inplace=True)\n",
    "# count the number of entries after duplicate removal\n",
    "entries_after_duplicate_removal = len(df)\n",
    "# print removed duplicate count\n",
    "print(f\"Removed {entries_before_duplicate_removal - entries_after_duplicate_removal} duplicate entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the string lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the tags column before parsing\n",
    "print(f\"The type of values in the Tags column is {type(df.loc[0, 'Tags'])}.\")\n",
    "# convert the raw string values of the Tags column to lists of strings\n",
    "df[\"Tags\"] = df[\"Tags\"].apply(parse_list_from_string)\n",
    "# show the tags column after parsing\n",
    "print(f\"The type of values in the Tags column is {type(df.loc[0, 'Tags'])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define article loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article by title\n",
    "def load_article(title, load_contents=True):\n",
    "    # create file name from title\n",
    "    file_name = f\"{title}.txt\"\n",
    "    # get the path of the article\n",
    "    file_path = os.path.join(articles_path, file_name)\n",
    "    # read the article\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    # return the contents of the article if requested\n",
    "    if load_contents:\n",
    "        # read the contents of the article\n",
    "        contents = file.read()\n",
    "        # close the file\n",
    "        file.close()\n",
    "        # return the contents of the article\n",
    "        return contents\n",
    "    # close the file\n",
    "    file.close()\n",
    "    # return the article path\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove articles which cannot be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize error count to 0 \n",
    "err_count = 0\n",
    "\n",
    "# iterate over dataset with index\n",
    "for index, row in df.iterrows():\n",
    "    # get the title of the article\n",
    "    title = row[\"Title\"]\n",
    "    try:\n",
    "        # attempt to load the article\n",
    "        article = load_article(title)\n",
    "    except:\n",
    "        # if the article cannot be loaded, increment the error count\n",
    "        err_count += 1\n",
    "        # remove row from dataset\n",
    "        df.drop(index, inplace=True)\n",
    "        # continue to next row\n",
    "        continue\n",
    "\n",
    "# reset the index of the main dataset\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print the number of errors\n",
    "print(f\"{err_count} files could not be loaded by title!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define article cleaning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the small english spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace characters like é, ë, ï, etc. with their corresponding characters\n",
    "def remove_accented_characters(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all newline characters\n",
    "def remove_newlines(text):\n",
    "    text = text.replace(\"\\n\\n\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\\r\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace shortened grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace shortened grammar with full grammar\n",
    "def replace_grammar(text):\n",
    "    text = text.replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\"he's\", \"he is\")\n",
    "    text = text.replace(\"she's\", \"she is\")\n",
    "    text = text.replace(\"'s\", \" its\")\n",
    "    text = text.replace(\"'t\", \" not\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    text = text.replace(\"'ve\", \" have\")\n",
    "    text = text.replace(\"'d\", \" would\")\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove double whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove double whitespace characters\n",
    "def remove_double_whitespace(text):\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove noise from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise from text\n",
    "def remove_noise(text):\n",
    "    # remove newline characters\n",
    "    text = remove_newlines(text)\n",
    "    # replace short grammar with full grammar\n",
    "    text = replace_grammar(text)\n",
    "    # remove accented characters\n",
    "    text = remove_accented_characters(text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # remove digits\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # remove double whitespace characters\n",
    "    text = remove_double_whitespace(text)\n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    # return the cleaned text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "def tokenize(text):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words from the text\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the text\n",
    "def lemmatize(tokens):\n",
    "    return [token.lemma_ for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main preprocessing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main preprocessing method which calls all cleaning methods\n",
    "def preprocess_text(text):\n",
    "    text = remove_noise(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stop_words(tokens)\n",
    "    tokens = lemmatize(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### commented for now - Preprocess all articles and append them to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over every row with index\n",
    "for index, row in df.iterrows():\n",
    "    # get the current title\n",
    "    title = row[\"Title\"]\n",
    "    # get the current text\n",
    "    article_contents = load_article(title)\n",
    "    # preprocess the text\n",
    "    article_contents = preprocess_text(article_contents)\n",
    "    # join all tokens together\n",
    "    article_contents = \" \".join(article_contents)\n",
    "    # make the text lowercase\n",
    "    article_contents = article_contents.lower()\n",
    "    # add the preprocessed text to the dataset\n",
    "    df.loc[index, \"Preprocessed_Text\"] = article_contents\n",
    "    # show the progress\n",
    "    print(f\"Preprocessed {index + 1} of {len(df)} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporary visualization of the preprocessed data of one article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the article index\n",
    "article_index = 1234\n",
    "# get the title of an article\n",
    "title = df[\"Title\"][article_index]\n",
    "# load the body of the article\n",
    "article_contents = load_article(title)\n",
    "# preprocess the text\n",
    "article_contents = preprocess_text(article_contents)\n",
    "# show the progress\n",
    "total_text = \" \".join(article_contents)\n",
    "print(total_text)\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# wc=WordCloud(max_font_size=60).generate(total_text)\n",
    "# plt.figure(figsize=(16, 12))\n",
    "# plt.imshow(wc, interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define one hot encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from a 2D array of strings\n",
    "def get_unique_value_frequency(df_column):\n",
    "    # create a dictionary to store the unique values\n",
    "    unique_values = {}\n",
    "    # iterate over the column\n",
    "    for value_list in df_column:\n",
    "        # iterate over the values in the list\n",
    "        for value in value_list:\n",
    "            if value not in unique_values:\n",
    "                # if the value is not in the dictionary, add it\n",
    "                unique_values[value] = 1\n",
    "            else:\n",
    "                # if the value is in the dictionary, increment the value\n",
    "                unique_values[value] += 1\n",
    "    # return the dictionary of unique values\n",
    "    return unique_values\n",
    "\n",
    "# check if a list contains a certain word and returns a binary boolean value\n",
    "def list_has_word(l, word):\n",
    "    return word in l and 1 or 0\n",
    "\n",
    "# one hot encode a dataframe's column that contains lists of strings in each value\n",
    "def custom_one_hot_encoding(df, column_name, prefix=None, prefix_sep=\"_\"):\n",
    "    # create a dictionary to store the one hot encoded columns\n",
    "    one_hot_encoded_columns = {}\n",
    "    # get the unique values from the column\n",
    "    unique_values = get_unique_value_frequency(df[column_name])\n",
    "    # iterate over the unique values\n",
    "    for unique_value in unique_values:\n",
    "        # create a clean string of the unique value\n",
    "        clean_unique_value = unique_value.replace(\" \", \"_\")\n",
    "        # create a new column name\n",
    "        new_column_name = prefix and f\"{prefix}{prefix_sep}{clean_unique_value}\" or f\"{column_name}{prefix_sep}{clean_unique_value}\"\n",
    "        # one hot encode the column using the current unique value\n",
    "        ohe_list = df[column_name].apply(lambda l: list_has_word(l, unique_value))\n",
    "        # add the new list to the dictionary\n",
    "        one_hot_encoded_columns[new_column_name] = ohe_list\n",
    "    # return a new dataframe with the one hot encoded columns\n",
    "    return pd.DataFrame(one_hot_encoded_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute one hot encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the tags column of the dataframe\n",
    "ohe_tags_df = custom_one_hot_encoding(df, \"Tags\", \"tag\")\n",
    "# merge the one hot encoded tags dataframe with the main dataframe by index\n",
    "df = df.join(ohe_tags_df)\n",
    "# drop the tags column from the main dataframe\n",
    "df.drop(columns=[\"Tags\"], inplace=True)\n",
    "# delete the one hot encoded dataframe variable\n",
    "del ohe_tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose index number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article index number for which we want to see the text\n",
    "article_index = 7685"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InSight Crime Analysis\n",
      "\n",
      "The charges filed against the accused, the highest ranking military officials charged with links to organized crime in fifteen years, could deal a blow to President Calderon’s anti-organized crime legacy. \n",
      "\n",
      "On July 31, a federal judge in Toluca, Mexico State, charged six high-ranking members of the Mexican army, including the former second-highest ranking defense official and three retired generals, for ties to the Beltran Leyva Organization (BLO), reported Proceso. Six high-ranking members of the Mexican army, including four generals, have been formally charged with collaborating with drug traffickers in what is likely the biggest corruption case of Felipe Calderon’s presidency.\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "\n",
    "# Load the article\n",
    "title = df[\"Title\"][article_index]\n",
    "article_contents = load_article(title)\n",
    "preprocessed_text = preprocess_text(article_contents)\n",
    "total_text = \" \".join(preprocessed_text)\n",
    "\n",
    "# Define the article text into a variable and a nlp object\n",
    "doc = article_contents\n",
    "doc_nlp = nlp(doc)\n",
    "\n",
    "# Define the function to get the top n keywords\n",
    "keyword = []\n",
    "stopwords = list(STOP_WORDS)\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN','VERB']\n",
    "for token in doc_nlp:\n",
    "    if(token.text in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "    if(token.pos_ in pos_tag):\n",
    "        keyword.append(token.text)\n",
    "\n",
    "# Get the top 10 keywords\n",
    "freq_word = Counter(keyword)\n",
    "freq_word.most_common(10)\n",
    "\n",
    "# Get the weighted keywords\n",
    "max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "for word in freq_word.keys():\n",
    "    freq_word[word] = (freq_word[word]/max_freq)\n",
    "freq_word.most_common(10)\n",
    "\n",
    "# Set the weight for each sentence\n",
    "sent_strength = {}\n",
    "for sent in doc_nlp.sents:\n",
    "    for word in sent:\n",
    "        if word.text in freq_word.keys():\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent]+=freq_word[word.text]\n",
    "            else:\n",
    "                sent_strength[sent]=freq_word[word.text]\n",
    "\n",
    "# Compute and combine the weights of the sentences\n",
    "summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)\n",
    "final_sentences = [ w.text for w in summarized_sentences ]\n",
    "summary = ' '.join(final_sentences)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency from words with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('army', 1.0),\n",
       " ('crime', 0.875),\n",
       " ('Calderon', 0.75),\n",
       " ('organized', 0.75),\n",
       " ('retired', 0.625),\n",
       " ('ranking', 0.5),\n",
       " ('Mexican', 0.5),\n",
       " ('charged', 0.5),\n",
       " ('members', 0.375),\n",
       " ('including', 0.375)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the weighted keywords\n",
    "freq_word.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization using Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All photographs courtesy of AFP, EPA, Getty Images and Reuters']\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "# Configure model\n",
    "model_name = 'google/pegasus-xsum'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Set the article\n",
    "src_text = doc\n",
    "\n",
    "# Create a summary\n",
    "batch = tokenizer(src_text, truncation=True, padding='longest',return_tensors='pt')\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "# Print the summary\n",
    "print(tgt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get title and tags from article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mexican Generals Charged with Drug Trafficking\n",
      "\n",
      "['mexico', 'los monos', 'el salvador', 'bolivia', 'colombia personalities', 'beltran leyva org']\n"
     ]
    }
   ],
   "source": [
    "# Print the title of the article based on the index number\n",
    "print(df[\"Title\"][article_index])\n",
    "\n",
    "print()\n",
    "\n",
    "# Print the tags of the article based on the index number\n",
    "print(df[\"Tags\"][article_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Six high-ranking members of the Mexican army, including four generals, have been formally charged with collaborating with drug traffickers in what is likely the biggest corruption case of Felipe Calderon’s presidency.\n",
      "\n",
      "On July 31, a federal judge in Toluca, Mexico State, charged six high-ranking members of the Mexican army, including the former second-highest ranking defense official and three retired generals, for ties to the Beltran Leyva Organization (BLO), reported Proceso. The judge also charged four members of the BLO, including US-born Hector Valdez Villareal, alias “La Barbie,” whose testimony is thought to have implicated the army officials.\n",
      "\n",
      "Four of the six accused have been detained since May under the pre-trial detention process known as “arraigo,” which allows prosecutors to hold individuals suspected of participating in organized crime for up to 80 days. The four are: former Assistant Secretary of Defense Tomas Angeles Dauahare (pictured), retired Divisional General Ricardo Escorcia Vargas, retired Brigadier General Roberto Dawe Gonzalez, and retired Lieutenant Colonel Silvio Isidro de Jesus Hernandez Soto. The warrant for their formal arrest was issued just three days before the accused were to be released from pre-trial detention. Following the issue of the warrant, they were moved to a maximum security federal prison in Toluca.\n",
      "\n",
      "Charges were also levied against Major Iván Reyna Muñoz, who served as a witness in the initial inquiry against General Dauahare.\n",
      "\n",
      "InSight Crime Analysis\n",
      "\n",
      "The charges filed against the accused, the highest ranking military officials charged with links to organized crime in fifteen years, could deal a blow to President Calderon’s anti-organized crime legacy. The army served as a pillar of Calderon’s militarized strategy against organized crime; over 50,000 soldiers have been deployed in 14 of Mexico’s 32 states since Calderon took office in 2006. The case against Angeles is especially disastrous for Calderon given that the retired general was key in implementing the president’s anti-crime strategy during his time as assistant secretary of defense from 2006 to 2008.\n",
      "\n",
      "Regardless of the danger these accusation pose to Calderon’s legacy, it remains to be seen whether the case against the generals will cause major damage to the army’s reputation and popularity among the Mexican public. Despite continued reports of unpunished human rights abuses by the military, the Mexican army continues to enjoy high levels of support, especially in comparison to other public institutions such as the police. A survey by the Pew Research Center published in June showed that 80 percent of Mexicans surveyed approved of using the army to fight organized crime.\n",
      "\n",
      "Before winning the presidential election in July, Enrique Peña Nieto declared his support for the continued deployment of the military and praised both the army and the navy’s success against organized crime. Peña Nieto takes power in December.\n"
     ]
    }
   ],
   "source": [
    "# Print the article contents if necessary\n",
    "print(article_contents)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc03c57ad7b86a0ef884a1652e2e233be898816187802410f759e457b32a702a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
