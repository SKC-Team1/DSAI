{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "# get the dataset path from the environment variables\n",
    "dataset_path = os.environ.get(\"DATASET_PATH\")\n",
    "# get the articles path for the known publisher\n",
    "articles_path = os.path.join(dataset_path, \"articles\")\n",
    "# load the dataset into the notebook\n",
    "df = pd.read_csv(f\"{dataset_path}/article_info_V2.csv\", index_col=0, parse_dates=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(str):\n",
    "    # make the string lowercase\n",
    "    str = str.lower()\n",
    "    # remove all non-alphanumeric characters\n",
    "    str = re.sub(r\"[^\\w\\s]\", \"\", str)\n",
    "    # remove the leading and trailing spaces\n",
    "    return str.strip()\n",
    "\n",
    "def parse_list_from_string(str):\n",
    "    # split the string on each comma\n",
    "    raw_list = str.split(\",\")\n",
    "    # clean every string in the list\n",
    "    return list(map(clean_string, raw_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unused columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the author, type and keywords columns\n",
    "df.drop(columns=[\"Author\", \"Type\", \"Keywords\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with empty date values\n",
    "df.dropna(subset=[\"Date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the external dataset\n",
    "external_df = pd.read_csv(f\"{dataset_path}/other-articles.csv\", index_col=0, parse_dates=[1], sep=\";\")\n",
    "# remove the url column from the external dataset\n",
    "external_df.drop(columns=[\"Url\"], inplace=True)\n",
    "# add the external dataset to the main dataset\n",
    "df = pd.concat([df, external_df])\n",
    "# reset the index of the main dataset\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# clear the external dataset variable\n",
    "del external_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of entries before duplicate removal\n",
    "entries_before_duplicate_removal = len(df)\n",
    "# remove duplicate entries by title and date\n",
    "df.drop_duplicates(subset=[\"Title\", \"Date\"], keep=\"first\", inplace=True)\n",
    "# count the number of entries after duplicate removal\n",
    "entries_after_duplicate_removal = len(df)\n",
    "# print removed duplicate count\n",
    "print(f\"Removed {entries_before_duplicate_removal - entries_after_duplicate_removal} duplicate entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the string lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the tags column before parsing\n",
    "print(f\"The type of values in the Tags column is {type(df.loc[0, 'Tags'])}.\")\n",
    "# convert the raw string values of the Tags column to lists of strings\n",
    "df[\"Tags\"] = df[\"Tags\"].apply(parse_list_from_string)\n",
    "# show the tags column after parsing\n",
    "print(f\"The type of values in the Tags column is {type(df.loc[0, 'Tags'])}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define article loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article by title\n",
    "def load_article(title, load_contents=True):\n",
    "    # create file name from title\n",
    "    file_name = f\"{title}.txt\"\n",
    "    # get the path of the article\n",
    "    file_path = os.path.join(articles_path, file_name)\n",
    "    # read the article\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    # return the contents of the article if requested\n",
    "    if load_contents:\n",
    "        # read the contents of the article\n",
    "        contents = file.read()\n",
    "        # close the file\n",
    "        file.close()\n",
    "        # return the contents of the article\n",
    "        return contents\n",
    "    # close the file\n",
    "    file.close()\n",
    "    # return the article path\n",
    "    return file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove articles which cannot be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize error count to 0 \n",
    "err_count = 0\n",
    "\n",
    "# iterate over dataset with index\n",
    "for index, row in df.iterrows():\n",
    "    # get the title of the article\n",
    "    title = row[\"Title\"]\n",
    "    try:\n",
    "        # attempt to load the article\n",
    "        article = load_article(title)\n",
    "    except:\n",
    "        # if the article cannot be loaded, increment the error count\n",
    "        err_count += 1\n",
    "        # remove row from dataset\n",
    "        df.drop(index, inplace=True)\n",
    "        # continue to next row\n",
    "        continue\n",
    "\n",
    "# reset the index of the main dataset\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print the number of errors\n",
    "print(f\"{err_count} files could not be loaded by title!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define article cleaning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load spacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the small english spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace characters like é, ë, ï, etc. with their corresponding characters\n",
    "def remove_accented_characters(text):\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing newline characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all newline characters\n",
    "def remove_newlines(text):\n",
    "    text = text.replace(\"\\n\\n\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\\r\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace shortened grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace shortened grammar with full grammar\n",
    "def replace_grammar(text):\n",
    "    text = text.replace(\"it's\", \"it is\")\n",
    "    text = text.replace(\"he's\", \"he is\")\n",
    "    text = text.replace(\"she's\", \"she is\")\n",
    "    text = text.replace(\"'s\", \" its\")\n",
    "    text = text.replace(\"'t\", \" not\")\n",
    "    text = text.replace(\"'re\", \" are\")\n",
    "    text = text.replace(\"'ll\", \" will\")\n",
    "    text = text.replace(\"'ve\", \" have\")\n",
    "    text = text.replace(\"'d\", \" would\")\n",
    "    text = text.replace(\"'m\", \" am\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove double whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove double whitespace characters\n",
    "def remove_double_whitespace(text):\n",
    "    return re.sub(r\"\\s\\s+\", \" \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove noise from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise from text\n",
    "def remove_noise(text):\n",
    "    # remove newline characters\n",
    "    text = remove_newlines(text)\n",
    "    # replace short grammar with full grammar\n",
    "    text = replace_grammar(text)\n",
    "    # remove accented characters\n",
    "    text = remove_accented_characters(text)\n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    # remove digits\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # remove double whitespace characters\n",
    "    text = remove_double_whitespace(text)\n",
    "    # lowercase the text\n",
    "    text = text.lower()\n",
    "    # return the cleaned text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "def tokenize(text):\n",
    "    return nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words from the text\n",
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the text\n",
    "def lemmatize(tokens):\n",
    "    return [token.lemma_ for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main preprocessing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main preprocessing method which calls all cleaning methods\n",
    "def preprocess_text(text):\n",
    "    text = remove_noise(text)\n",
    "    tokens = tokenize(text)\n",
    "    tokens = remove_stop_words(tokens)\n",
    "    tokens = lemmatize(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### commented for now - Preprocess all articles and append them to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over every row with index\n",
    "for index, row in df.iterrows():\n",
    "    # get the current title\n",
    "    title = row[\"Title\"]\n",
    "    # get the current text\n",
    "    article_contents = load_article(title)\n",
    "    # preprocess the text\n",
    "    article_contents = preprocess_text(article_contents)\n",
    "    # join all tokens together\n",
    "    article_contents = \" \".join(article_contents)\n",
    "    # make the text lowercase\n",
    "    article_contents = article_contents.lower()\n",
    "    # add the preprocessed text to the dataset\n",
    "    df.loc[index, \"Preprocessed_Text\"] = article_contents\n",
    "    # show the progress\n",
    "    print(f\"Preprocessed {index + 1} of {len(df)} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temporary visualization of the preprocessed data of one article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the article index\n",
    "article_index = 1234\n",
    "# get the title of an article\n",
    "title = df[\"Title\"][article_index]\n",
    "# load the body of the article\n",
    "article_contents = load_article(title)\n",
    "# preprocess the text\n",
    "article_contents = preprocess_text(article_contents)\n",
    "# show the progress\n",
    "total_text = \" \".join(article_contents)\n",
    "print(total_text)\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "# import matplotlib.pyplot as plt\n",
    "# wc=WordCloud(max_font_size=60).generate(total_text)\n",
    "# plt.figure(figsize=(16, 12))\n",
    "# plt.imshow(wc, interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define one hot encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values from a 2D array of strings\n",
    "def get_unique_value_frequency(df_column):\n",
    "    # create a dictionary to store the unique values\n",
    "    unique_values = {}\n",
    "    # iterate over the column\n",
    "    for value_list in df_column:\n",
    "        # iterate over the values in the list\n",
    "        for value in value_list:\n",
    "            if value not in unique_values:\n",
    "                # if the value is not in the dictionary, add it\n",
    "                unique_values[value] = 1\n",
    "            else:\n",
    "                # if the value is in the dictionary, increment the value\n",
    "                unique_values[value] += 1\n",
    "    # return the dictionary of unique values\n",
    "    return unique_values\n",
    "\n",
    "# check if a list contains a certain word and returns a binary boolean value\n",
    "def list_has_word(l, word):\n",
    "    return word in l and 1 or 0\n",
    "\n",
    "# one hot encode a dataframe's column that contains lists of strings in each value\n",
    "def custom_one_hot_encoding(df, column_name, prefix=None, prefix_sep=\"_\"):\n",
    "    # create a dictionary to store the one hot encoded columns\n",
    "    one_hot_encoded_columns = {}\n",
    "    # get the unique values from the column\n",
    "    unique_values = get_unique_value_frequency(df[column_name])\n",
    "    # iterate over the unique values\n",
    "    for unique_value in unique_values:\n",
    "        # create a clean string of the unique value\n",
    "        clean_unique_value = unique_value.replace(\" \", \"_\")\n",
    "        # create a new column name\n",
    "        new_column_name = prefix and f\"{prefix}{prefix_sep}{clean_unique_value}\" or f\"{column_name}{prefix_sep}{clean_unique_value}\"\n",
    "        # one hot encode the column using the current unique value\n",
    "        ohe_list = df[column_name].apply(lambda l: list_has_word(l, unique_value))\n",
    "        # add the new list to the dictionary\n",
    "        one_hot_encoded_columns[new_column_name] = ohe_list\n",
    "    # return a new dataframe with the one hot encoded columns\n",
    "    return pd.DataFrame(one_hot_encoded_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute one hot encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the tags column of the dataframe\n",
    "ohe_tags_df = custom_one_hot_encoding(df, \"Tags\", \"tag\")\n",
    "# merge the one hot encoded tags dataframe with the main dataframe by index\n",
    "df = df.join(ohe_tags_df)\n",
    "# drop the tags column from the main dataframe\n",
    "df.drop(columns=[\"Tags\"], inplace=True)\n",
    "# delete the one hot encoded dataframe variable\n",
    "del ohe_tags_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose index number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_index = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Both men were high-ranking commanders who demobilized with the Revolutionary Armed Forces of Colombia (Fuerzas Armadas Revolucionarias de Colombia - FARC) in 2016 but later returned to arms three years later as part of the Second Marquetalia, a FARC dissident force led by Luciano Marín Arango, alias \"Iván Márquez.\"\n",
      "\n",
      "\n",
      "\n",
      "\"Colombia is freed from two symbols of evil, who belonged to the FARC, alias El Paisa y alias Romaña,\" said Molano, adding in an interview that an isolated Iván Márquez would have to surrender or \"end up like his companions. \n",
      "\n",
      "If the 10th Front was responsible for El Paisa and Romaña's deaths, it underscores that the Second Marquetalia has failed in one of its primary goals: uniting the disparate dissident factions that InSight Crime has come to label the ex-FARC Mafia. Colombia's Defense Minister has reported that top ex-FARC commanders El Paisa and Romaña were shot dead in Venezuela, making them the latest former FARC leaders killed in the neighboring country in 2021.\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from heapq import nlargest\n",
    "\n",
    "title = df[\"Title\"][article_index]\n",
    "article_contents = load_article(title)\n",
    "preprocessed_text = preprocess_text(article_contents)\n",
    "total_text = \" \".join(preprocessed_text)\n",
    "\n",
    "doc = article_contents\n",
    "doc_nlp = nlp(doc)\n",
    "\n",
    "keyword = []\n",
    "stopwords = list(STOP_WORDS)\n",
    "pos_tag = ['PROPN', 'ADJ', 'NOUN','VERB']\n",
    "for token in doc_nlp:\n",
    "    if(token.text in stopwords or token.text in punctuation):\n",
    "        continue\n",
    "    if(token.pos_ in pos_tag):\n",
    "        keyword.append(token.text)\n",
    "\n",
    "freq_word = Counter(keyword)\n",
    "freq_word.most_common(10)\n",
    "\n",
    "max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "for word in freq_word.keys():\n",
    "    freq_word[word] = (freq_word[word]/max_freq)\n",
    "freq_word.most_common(10)\n",
    "\n",
    "sent_strength = {}\n",
    "for sent in doc_nlp.sents:\n",
    "    for word in sent:\n",
    "        if word.text in freq_word.keys():\n",
    "            if sent in sent_strength.keys():\n",
    "                sent_strength[sent]+=freq_word[word.text]\n",
    "            else:\n",
    "                sent_strength[sent]=freq_word[word.text]\n",
    "\n",
    "summarized_sentences = nlargest(3, sent_strength, key=sent_strength.get)\n",
    "final_sentences = [ w.text for w in summarized_sentences ]\n",
    "summary = ' '.join(final_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization using Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The killing of two high-ranking dissident commanders from Colombia's largest rebel group, the Revolutionary Armed Forces of Colombia (FARC), is a huge blow to the dissident group.\"]\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "import torch\n",
    "\n",
    "# Configure model\n",
    "model_name = 'google/pegasus-xsum'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "src_text = doc\n",
    "\n",
    "batch = tokenizer(src_text, truncation=True, padding='longest',return_tensors='pt')\n",
    "translated = model.generate(**batch)\n",
    "tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "print(tgt_text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc03c57ad7b86a0ef884a1652e2e233be898816187802410f759e457b32a702a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
